# ğŸŒ¦ï¸ Weather Data Pipeline

This project creates a complete ETL pipeline using **OpenWeatherMap API**, **Apache Airflow**, **dbt**, and **PostgreSQL**. It fetches real-time weather data, transforms it using dimensional modeling principles (star schema), and loads it into a local Postgres data warehouse.

---

## ğŸ“¦ Tech Stack

- **Apache Airflow** â€“ ETL orchestration
- **dbt (Data Build Tool)** â€“ SQL transformations
- **PostgreSQL** â€“ Data warehouse
- **SQLAlchemy & Pandas** â€“ Raw data loading
- **Docker** â€“ Containerized setup
- **pytest** â€“ Unit testing
- **OpenWeatherMap API** â€“ Real-time weather source [openweathermap](https://openweathermap.org/)

---

## ğŸ“ Project Structure

â”œâ”€â”€ dags/  
â”‚   â”œâ”€â”€ weather_etl_dag.py         # Airflow DAG definition  
â”‚   â”œâ”€â”€ fetch_weather.py           # API data extraction script  
â”‚   â””â”€â”€ weather_raw.json           # Sample raw data  
â”‚
â”œâ”€â”€ dbt_project/  
â”‚   â”œâ”€â”€ models/  
â”‚   â”‚   â”œâ”€â”€ staging/  
â”‚   â”‚   â”‚   â””â”€â”€ stg_weather_raw.sql  
â”‚   â”‚   â”œâ”€â”€ marts/  
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_location.sql  
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_date.sql  
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_weather_condition.sql  
â”‚   â”‚   â”‚   â””â”€â”€ fact_weather.sql  
â”‚   â”‚   â””â”€â”€ tests/  
â”‚   â”‚   â”‚   â””â”€â”€ fact_weather_test.sql
â”‚   â”‚   â”‚   â””â”€â”€ test_extract.py    # Unit tests for fetch_weather.py
â”‚   â”œâ”€â”€ dbt_project.yml  
â”‚   â””â”€â”€ profiles.yml  
â”‚
â”œâ”€â”€ tests/  
â”‚   â””â”€â”€ test_extract.py            # Unit tests for fetch_weather.py  
â”‚
â”œâ”€â”€ docker-compose.yml  
â”œâ”€â”€ requirements.txt  
â””â”€â”€ README.md 

## Setup Instructions

1. **Clone the repository:**
  ```bash
  git clone https://github.com/kkarthikraja/airflow-docker.git
  cd airflow-docker
  ```

2. **Install required Python dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
   
3. **Set up your environment variables:**

   Set the OpenWeatherMap API key as an environment variable @[openweathermap](https://openweathermap.org/)

   Export the following:

  ```bash
  export OPENWEATHERMAP_API_KEY=<your_api_key>
  export DB_HOST=localhost
  export DB_NAME=weather_db
  export DB_USER=postgres
  export DB_PASSWORD=postgres
  ```
   
5. **Download and Install Docker Desktop:**
   Download and install Docker Desktop from https://www.docker.com/products/docker-desktop and ensure it's running in the background.

6. **Install PostgreSQL database:**
   Install PostgreSQL locally or use a Docker container. Make sure your credentials match the ones provided in the environment variables or ```bash profiles.yml ```.

7. **Set up your PostgreSQL credentials as environment variables:**
   ```bash
   pip install -r requirements.txt
   ```

8. **Run the Docker desktop on the background:**
   ```bash
   pip install -r requirements.txt
   ```

9. **Set up the dbt project:**

   Update your `profiles.yml` under the `dags/dbt_project/` folder with:
   
   ```yaml
  my_postgres:
    target: dev
    outputs:
      dev:
        type: postgres
        host: localhost
        user: postgres
        password: postgres
        dbname: weather_db
        schema: public
    ```


10. **Run the Airflow on your docker from executing on your root folder /docker-airflow/ :**

   ```bash
   docker exec -it airflow-docker-airflow-webserver-1 bashdocker exec -it airflow-docker-airflow-webserver-1 bash
   ``` 

11. **Run Airflow with Docker:**

   Build and start the Airflow Docker containers:

   Run the following command from the root folder:

  ```bash
  docker-compose up --build
  ```

   Access the Airflow UI at: [http://localhost:8080](http://localhost:8080) (Default login: `admin` / `admin`)

   
10. **Running the Data Pipeline:**

    The pipeline will automatically fetch weather data, perform transformation (using dbt), and load it into PostgreSQL.

    The Airflow DAG is scheduled to run every 60 minutes.

    Airflow will trigger dbt runs through the subprocess command.

11. **Testing:**

    You can add unit tests for the dbt models inside the /tests folder.

    The pipeline includes a fact_weather_test.sql file that checks for NULLs in foreign keys.

    Airflow will trigger dbt runs through the subprocess command.
   
11. **Exception Handling:**

    The API extraction step includes exception handling for failed requests.

    Any errors encountered during the pipeline execution will be logged by Airflow.
